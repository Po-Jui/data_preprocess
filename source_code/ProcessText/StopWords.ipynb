{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " author:jjk\n",
    " datetime:2019/11/26\n",
    " coding:utf-8\n",
    " project name:Pycharm_workstation\n",
    " Program function: 自定义去停用词\n",
    " \n",
    "\"\"\"\n",
    "import re,jieba,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文本信息\n",
    "def readFile(path):\n",
    "    str_doc = \"\"\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        str_doc = f.read() # 加载全文本\n",
    "    return str_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则对字符串清洗\n",
    "def textParse(str_doc):\n",
    "    # 去除字符\n",
    "    str_doc = re.sub('\\u3000','',str_doc)\n",
    "    # 去除空格\n",
    "    str_doc = re.sub('\\s+','',str_doc)\n",
    "    # 去除换行符\n",
    "    str_doc = str_doc.replace('\\n','')\n",
    "    \n",
    "    # 正则过滤掉特殊符号，标点，英文，数字等\n",
    "    r1 = '[a-zA-Z0-9’!\"#$%&\\'()*+,-./:：;；|<=>?@，—。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "    str_doc=re.sub(r1, ' ', str_doc)\n",
    "    return str_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建停用词表\n",
    "def get_stop_words(path=r'../dataSet/StopWord/NLPIR_stopwords.txt'):\n",
    "    file = open(path,'r',encoding='utf-8').read().split('\\n')\n",
    "    # print(type(set(file)))\n",
    "    return set(file) # 去除重复\n",
    "\n",
    "\n",
    "# 去除一些停用词和数组\n",
    "def rm_tokens(words,stwlist): # 切词的词典列表和对照停用词\n",
    "    words_list = list(words) # 切完词的结果转换为列表\n",
    "    stop_words = stwlist # 停用词\n",
    "    \n",
    "    for i in range(words_list.__len__())[::-1]: #words_list原始的文本词获取它的总长度，逆序遍历\n",
    "        if words_list[i] in stop_words: # 去除停用词\n",
    "            words_list.pop(i) # 弹出\n",
    "        elif words_list[i].isdigit(): # 去除数字\n",
    "            words_list.pop(i)\n",
    "        elif len(words_list[i]) ==1: # 去除单个字符\n",
    "            words_list.pop(i)\n",
    "        elif words_list[i] ==\" \": # 去除空字符\n",
    "            words_list.pop(i)\n",
    "    return words_list # 返回列表\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用jieba对文本进行分词，返回切词后的list\n",
    "def seg_doc(str_doc):\n",
    "    # 1、正则处理原文本\n",
    "    sent_list = str_doc.split('\\n')\n",
    "    sent_list = map(textParse,sent_list) # 获取的是一个列表\n",
    "    # 2 获取停用词\n",
    "    stwlist = get_stop_words()\n",
    "    # 3、分词并去除停用词\n",
    "    word_2dlist = [rm_tokens(jieba.cut(part,cut_all=False),stwlist) for part in sent_list] # 列表中每一句，精确进行分词,去掉一些停用词和数字\n",
    "    # 4、合并列表\n",
    "    word_list = sum(word_2dlist,[])\n",
    "    return word_list # 返回列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\JIAJIK~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.759 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['性感', '比基尼', '亮丽', '奥帆', '中心', '女郎', '亲吻', '阳光', '奥帆赛', '日到', '日于', '青岛', '举行', '级别', '项目', '比赛', '金牌', '截至', '下午', '所有', '奥帆赛', '帆船', '规模', '人数', '运动员', '总数', '已报', '运动队', '熟悉', '赛地', '加紧训练', '赛地', '检修', '帆船', '阳光', '女郎', '显得', '专业', '专业', '一份', '休闲', '美丽']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = r'../dataSet/CSCMNews/体育/2.txt'\n",
    "    str_doc = readFile(path)\n",
    "    # print(str_doc)\n",
    "    # 正则对字符串清洗\n",
    "    # print(textParse(str_doc))\n",
    "    word_list = seg_doc(str_doc)\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
